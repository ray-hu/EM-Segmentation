{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM_RF_EM_Segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ray-hu/EM-Segmentation/blob/master/SVM_RF_EM_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "e2Rx61m3rzRQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-peparation "
      ]
    },
    {
      "metadata": {
        "id": "1Lc8simysnGL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "metadata": {
        "id": "Ij4nGhAustMi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "94c9abef-8b08-4213-fc63-8b80e84312e7"
      },
      "cell_type": "code",
      "source": [
        "# install\n",
        "!pip install mahotas\n",
        "!pip install progressbar2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mahotas in /usr/local/lib/python3.6/dist-packages (1.4.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from mahotas) (1.14.6)\n",
            "Collecting progressbar2\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from progressbar2) (1.11.0)\n",
            "Collecting python-utils>=2.3.0 (from progressbar2)\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
            "Installing collected packages: python-utils, progressbar2\n",
            "Successfully installed progressbar2-3.38.0 python-utils-2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XSLjEwVmsy8b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "from glob import glob\n",
        "import argparse\n",
        "import os\n",
        "import progressbar\n",
        "import pickle as pkl\n",
        "from numpy.lib import stride_tricks\n",
        "from skimage import feature\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import mahotas as mt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FPbHEGiusNR0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mounting Google Drive locally"
      ]
    },
    {
      "metadata": {
        "id": "8yp717l6sOuS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "aeb32dac-418b-4ca6-b732-9d380e8a4c4c"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./GoogleDrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at ./GoogleDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MChJL-WosW8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ln -s GoogleDrive/My\\ Drive ./drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MS99fq77sZsN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_q0cwPz-lCdD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Extract Features from the images\n",
        "\n",
        "**Feature Vector:**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1.  ** Spectral(RGB)**\n",
        "\n",
        "\n",
        "*   Red\n",
        "*   Green\n",
        "*   Blue\n",
        "\n",
        "\n",
        "2.   **Texture**\n",
        "\n",
        "\n",
        "*   Local binary pattern(LBP)\n",
        "\n",
        "\n",
        "\n",
        "3.  ** Haralick's texture features**\n",
        "\n",
        "\n",
        "*  Angular second moment\n",
        "* Contrast\n",
        "* Correlation\n",
        "* Sum of Square: variance\n",
        "* Inverse difference moment\n",
        "* Sum average\n",
        "* Sum variance\n",
        "* Sum entropy\n",
        "* Entropy\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "97Vl7utxrwFz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ]
    },
    {
      "metadata": {
        "id": "XEJP4cEOru6I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "a9b6577b-3b5e-4443-af22-cf097f7ceedd"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "#from skimage import io\n",
        "import glob\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_data_set(load_aug = False):\n",
        "  if load_aug:\n",
        "    imgs_p = glob.glob(\"drive/data/aug/train/*.tif\")\n",
        "    labels_p = glob.glob(\"drive/data/aug/label/*.tif\")\n",
        "    tests_p = glob.glob(\"drive/data/test/*.tif\")\n",
        "  else:\n",
        "    imgs_p = glob.glob(\"drive/data/train/*.tif\")\n",
        "    labels_p = glob.glob(\"drive/data/label/*.tif\")\n",
        "    tests_p = glob.glob(\"drive/data/test/*.tif\")\n",
        "  imgs_train = []\n",
        "  label_train = []\n",
        "  imgs_test = []\n",
        "  print(\"_\"*30)\n",
        "  print(\"[INFO] Load orginal training data...\")\n",
        "  for img_p in imgs_p:\n",
        "    imgs_train.append(cv2.imread(img_p, 1))\n",
        "  for label_p in labels_p:\n",
        "    label_train.append(cv2.imread(label_p,0))\n",
        "    \n",
        "  print(\"check the consistency of training data...\")\n",
        "  print(\"num of imgs_train: {}\".format(len(imgs_train)))\n",
        "  print('num of groundTruth_train: {}'.format(len(label_train)))\n",
        "  print(\"_\"*30)\n",
        "  print(\"[INFO] Load orginal testing data...\")\n",
        "  for test_p in tests_p:\n",
        "    imgs_test.append(cv2.imread(test_p,1))\n",
        "  print('num of test images: {}'.format(len(imgs_test)))\n",
        "  return imgs_train, label_train, imgs_test\n",
        "\n",
        "imgs_train, label_train, imgs_test = load_data_set()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "______________________________\n",
            "[INFO] Load orginal training data...\n",
            "check the consistency of training data...\n",
            "num of imgs_train: 30\n",
            "num of groundTruth_train: 30\n",
            "______________________________\n",
            "[INFO] Load orginal testing data...\n",
            "num of test images: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6kXyVs9e6Ghe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Extract Features"
      ]
    },
    {
      "metadata": {
        "id": "bQ_3jEj19iFy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from skimage import feature\n",
        "from numpy.lib import stride_tricks\n",
        "import mahotas as mt\n",
        "import progressbar\n",
        "\n",
        "def extract_features(img, img_gary, label, train):\n",
        "  #1.  RGB values have beed extracted when loading the orignal dataset\n",
        "  \n",
        "  #2.  Local Binary Pattern\n",
        "  def LBP(img,points,radius):\n",
        "    '''\n",
        "     points: Number of circularly symmetric neighbour set points \n",
        "     radius: Radius of circle\n",
        "    '''\n",
        "    print(' extracting local binary pattern features.')\n",
        "    lbp = feature.local_binary_pattern(img, points, radius)\n",
        "    return (lbp-np.min(lbp))/(np.max(lbp)-np.min(lbp)) * 255\n",
        "  \n",
        "  #3. Haralick's texture features\n",
        "  def HTF(img, h_neighbors, ss_dix):\n",
        "    '''\n",
        "    h_neighbors:\n",
        "    '''\n",
        "    print(' extracting haralick texture features.')\n",
        "    size = h_neighbors\n",
        "    shape = (img.shape[0] - size + 1, img.shape[1] - size + 1, size, size)\n",
        "    strides = 2 * img.strides\n",
        "    patches = stride_tricks.as_strided(img, shape=shape, strides = strides)\n",
        "    patches = patches.reshape(-1,size,size)\n",
        "    \n",
        "    if len(ss_idx) == 0:\n",
        "      bar = progressbar.ProgressBar(maxval = len(patches), \\\n",
        "                                   widgets = [progressbar.Bar('=','[', ']'), ' ', progressbar.Percentage()])\n",
        "    else:\n",
        "      bar = progressbar.ProgressBar(maxval=len(ss_idx), \\\n",
        "        widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
        "    \n",
        "    \n",
        "    def calculate_haralick(img):\n",
        "      '''\n",
        "      calculate haralick features for each patch\n",
        "      '''\n",
        "      features = []\n",
        "      \n",
        "      feature_ = mt.features.haralick(img)\n",
        "      mean_ = feature_.mean(axis=0)\n",
        "      \n",
        "      [features.append(i) for i in mean_[0:9]]\n",
        "      \n",
        "      return np.array(features)\n",
        "    \n",
        "   \n",
        "    bar.start()\n",
        "    h_features = []\n",
        "    \n",
        "    if len(ss_idx) == 0:\n",
        "      for i, p in enumerate(patches):\n",
        "        bar.update(i+1)\n",
        "        h_features.append(calculate_haralick(p))\n",
        "    else:\n",
        "      for i, p in enumerate(patches[ss_idx]):\n",
        "        bar.update(i+1)\n",
        "        h_features.append(calculate_haralick(p))\n",
        "    \n",
        "    return np.array(h_features)\n",
        "  \n",
        "  \n",
        "  #4. extracting all the features\n",
        "  \n",
        "  # Hyperparameters:\n",
        "  lbp_radius = 24 # local binary pattern neighbourhood\n",
        "  lbp_points = lbp_radius*8 # number of circularly symmetric neighbour set points \n",
        "  h_neighbors = 11 # haralick neighbourhood\n",
        "  \n",
        "  \n",
        "  num_examples = 1000 # number of examples per image to use for training model\n",
        "  h_index = int((h_neighbors -1)/2)\n",
        "  \n",
        "  feature_img = np.zeros((img.shape[0], img.shape[1],4))\n",
        "  feature_img[:,:,:3] = img\n",
        "  feature_img[:,:,3] = LBP(img_gary, lbp_points, lbp_radius)\n",
        "  feature_img = feature_img[h_index:-h_index, h_index:-h_index]\n",
        "  s = feature_img.shape\n",
        "  features = feature_img.reshape((s[0]*s[1], s[2]))\n",
        "  \n",
        "  if train:\n",
        "    ss_idx = np.random.randint(0,features.shape[0],num_examples)\n",
        "    features = features[ss_idx]\n",
        "  else:\n",
        "    ss_idx = []\n",
        "  \n",
        "  h_features = HTF(img_gary, h_neighbors, ss_idx)\n",
        "  features = np.hstack((features, h_features))\n",
        "  \n",
        "  if train:\n",
        "    label = label[h_index:-h_index, h_index:-h_index]\n",
        "    labels = label.reshape(label.shape[0]*label.shape[1],1)\n",
        "    labels = labels[ss_idx]\n",
        "  else:\n",
        "    labels = None\n",
        " \n",
        "  return features, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lrr1GgchFBx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Generate new training dataset"
      ]
    },
    {
      "metadata": {
        "id": "3NSrFkeZ4nqD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_training_data(imgs_train, label_train,train):\n",
        "  print(\"_\"*30)\n",
        "  print(\"[INFO] Extracting features from original image(s)\")\n",
        "  \n",
        "  Features = []\n",
        "  Labels = []\n",
        "  \n",
        "  for i, img in enumerate(imgs_train):\n",
        "    img_gary = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    features, labels = extract_features(img,img_gary,label_train[i], train = train)\n",
        "    Features.append(features)\n",
        "    Labels.append(labels)\n",
        "    \n",
        "  Features = np.array(Features)\n",
        "  s = Features.shape\n",
        "  Features = Features.reshape((s[0]*s[1],s[2]))\n",
        "  Labels = np.array(Labels)\n",
        "  s = Labels.shape\n",
        "  Labels = Labels.reshape((s[0]*s[1],s[2]))\n",
        "  \n",
        "  print(\"_\"*30)\n",
        "  print(\"[INFO] check the size of generated training data\")\n",
        "  print('size of the feature vector: {}'.format(Features.shape))\n",
        "  print('size of the label vertor: {}'.format(Labels.shape))\n",
        "  \n",
        "  return Features, Labels\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pnyRRvpdFMcH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training"
      ]
    },
    {
      "metadata": {
        "id": "grUZujakPaa3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def train(train_X, train_Y, classifier):\n",
        "  \n",
        "  if classifier == \"SVM\":\n",
        "    print(\"_\"*30)\n",
        "    print ('[INFO] Training a Support Vector Machine model.')\n",
        "    model = SVC()\n",
        "  elif classifier == \"RF\":\n",
        "    print(\"_\"*30)\n",
        "    print('[INFO] Training a Random Forest model.')\n",
        "    model = RandomForestClassifier(n_estimators=250, max_depth=12, random_state=42)\n",
        "  elif classifier == \"GBC\":\n",
        "    model = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "    \n",
        "    \n",
        "  model.fit(train_X, train_Y)\n",
        "  filename = 'drive/data/'+ classifier + '_model.pkl'\n",
        "  with open(filename, 'wb') as file:  \n",
        "    pickle.dump(model, file)\n",
        "  \n",
        "  print(\"_\"*30)\n",
        "  print('[INFO] Training complete.')\n",
        "  print('training accuracy: %.f' %model.score(train_X,train_Y))\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "riHs81JjSJXw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "imgs_train, label_train, imgs_test = load_data_set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xO9jISHuUGc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "956f1a44-f07c-4a08-d6ff-27b8c109a429"
      },
      "cell_type": "code",
      "source": [
        "label= np.array(label_train[0])\n",
        "label.shape"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 512)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "eeke1AEHT817",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "27b19550-b215-4505-eb0a-03136b685fcf"
      },
      "cell_type": "code",
      "source": [
        "X, Y = generate_training_data(imgs_train, label_train,train=True)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "______________________________\n",
            "[INFO] Extracting features from original image(s)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[========================================================================] 100%"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "______________________________\n",
            "[INFO] check the size of generated training data\n",
            "size of the feature vector: (30000, 13)\n",
            "size of the label vertor: (30000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "idhfh2StXXEq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_X, test_X, train_Y, test_Y = train_test_split(X,Y,test_size=0.2, random_state=42) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sfqz-AMlWzZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "070a48e7-8034-4746-af2d-8e039c9578d4"
      },
      "cell_type": "code",
      "source": [
        "model = train(train_X, train_Y, classifier='RF')"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] Training a Random Forest model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO] Training complete.\n",
            "training accuracy: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cx8nFT-zFlkS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ]
    },
    {
      "metadata": {
        "id": "TT-lzSGCY5IU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test(test_X, test_Y, classifier):\n",
        "  \n",
        "  # load trained model\n",
        "  pkl_filename = filename = 'drive/data/'+ classifier + '_model.pkl'\n",
        "  with open(pkl_filename, 'rb') as file:  \n",
        "    model = pickle.load(file)\n",
        "  \n",
        "  # predict\n",
        "  pred = model.predict(test_X)\n",
        "  precision = metrics.precision_score(test_Y, pred, average='weighted', labels=np.unique(pred))\n",
        "  recall = metrics.recall_score(test_Y, pred, average='weighted', labels=np.unique(pred))\n",
        "  f1 = metrics.f1_score(test_Y, pred, average='weighted', labels=np.unique(pred))\n",
        "  accuracy = metrics.accuracy_score(test_Y, pred)\n",
        "  print ('_'*30)\n",
        "  print('[RESULTS] Testing...')\n",
        "  print ('Accuracy: %.2f' %accuracy)\n",
        "  print ('Precision: %.2f' %precision)\n",
        "  print ('Recall: %.2f' %recall)\n",
        "  print ('F1: %.2f' %f1)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9yYSyvPyZLDx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test(test_X, test_Y, classifier = 'SVM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "auaJQzew2KNO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tuning Hyperparameters\n",
        "\n",
        "#### Hyperparameter for SVM\n",
        "1. C: regularization patameter\n",
        "2. kernel: 'linear', 'poly', 'rbf', sigmoid, ...\n",
        "3. degree: degree of 'poly' kernel\n",
        "4. gamma: kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
        "\n",
        "#### Hyperparameter for RandomForest\n",
        "1. n_estimators: num of trees in the forest\n",
        "2. criterion: function to measure the quality of a split, 'gini' or 'entropy'\n",
        "3. max_depth: max depth of the tree\n",
        "4. min_samples_split: min num of samples to split an internal node\n",
        "5. min_samples_leaf: min num of samples to be at a leaf node\n",
        "6. max_features: num of features to consider when looking for the best split\n",
        "7. bootstrap: method for sampling data points(with or without replacement)\n",
        "\n",
        "#### Hyperparameter for feature extraction\n",
        "1. lbp_radius: local binary pattern neighborhood\n",
        "2. h_neighbors: haralick neighborhood\n",
        "3. num_examples: num of examples per image to use for training\n",
        "\n",
        "#### Other parameters related to above\n",
        "1. lbp_points = lbp_radius*8\n",
        "2. h_index = int((h_neighbors -1)/2)"
      ]
    },
    {
      "metadata": {
        "id": "27jbg2v89wtc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameters for RandomForest"
      ]
    },
    {
      "metadata": {
        "id": "3Z-VN8jI5scP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def train_tune_para_RF(train_X, train_Y):\n",
        "  \n",
        "  print(\"_\"*30)\n",
        "  print('[INFO] Tuning hyperparameters for randomforest model.')\n",
        "  \n",
        "  # Number of trees in random forest\n",
        "  n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "  # Criterion\n",
        "  criterion = ['gini', 'entropy']\n",
        "  # Number of features to consider at every split\n",
        "  max_features = ['auto', 'sqrt']\n",
        "  # Maximum number of levels in tree\n",
        "  max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "  max_depth.append(None)\n",
        "  # Minimum number of samples required to split a node\n",
        "  min_samples_split = [2, 5, 10]\n",
        "  # Minimum number of samples required at each leaf node\n",
        "  min_samples_leaf = [1, 2, 4]\n",
        "  # Method of selecting samples for training each tree\n",
        "  bootstrap = [True, False]\n",
        "  # Create the random grid\n",
        "  random_grid = {'n_estimators': n_estimators,\n",
        "                 'max_features': max_features,\n",
        "                 'max_depth': max_depth,\n",
        "                 'min_samples_split': min_samples_split,\n",
        "                 'min_samples_leaf': min_samples_leaf,\n",
        "                 'bootstrap': bootstrap}\n",
        "  pprint(random_grid)\n",
        "  # Use the random grid to search for best hyperparameters\n",
        "  # First create the base model to tune\n",
        "  rf = RandomForestClassifier()\n",
        "  # Random search of parameters, using 3 fold cross validation, \n",
        "  # search across 100 different combinations, and use all available cores\n",
        "  rf_search = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "  # Fit the random search model\n",
        "  rf_search.fit(train_X, train_Y)\n",
        "  \n",
        "  filename = 'drive/data/RF_search_model.pkl'\n",
        "  with open(filename, 'wb') as file:\n",
        "    pickle.dump(rf_search, file)\n",
        "    \n",
        "  pkl_filename = 'drive/data/RF_search_model.pkl' \n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "    rf_search_model = pickle.load(file)\n",
        "\n",
        "  print(\"_\"*30)\n",
        "  print(\"[RESULTS] After tuning\")\n",
        "  print('best hyperparameters:')\n",
        "  pprint(rf_search_model.best_params_)\n",
        "  # use these model to train the model again or :\n",
        "  best_model = rf_search_model.best_estimator_\n",
        "  best_accuracy = evaluate(best_model, test_X, test_Y)\n",
        "  print('accuracy = {}'.format(best_accuracy))\n",
        "  \n",
        "  return rf_search"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgeCLaKKFbXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tuning hyperparameters for SVM"
      ]
    },
    {
      "metadata": {
        "id": "BJrgFral-KY-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pprint\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "def train_tune_para_SVM(train_X, train_Y):\n",
        "  \n",
        "  print(\"_\"*30)\n",
        "  print('[INFO] Tuning hyperparameters for SVM model.')\n",
        "  \n",
        "  # C: regularization patameter\n",
        "  \n",
        "  C = np.linspace(0,1,num=10)\n",
        "  # kernel: 'linear', 'poly', 'rbf', sigmoid, ...\n",
        "  kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "  # degree: degree of 'poly' kernel\n",
        "  degree = [3,10,30,100,300]\n",
        "  # gamma: kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
        "  \n",
        "  # Create the random grid\n",
        "  random_grid = {'C': C,\n",
        "                 'kernel': kernel,\n",
        "                 'degree': degree}\n",
        "  pprint(random_grid)\n",
        "  # Use the random grid to search for best hyperparameters\n",
        "  # First create the base model to tune\n",
        "  estimator = SVC()\n",
        "  # Random search of parameters, using 3 fold cross validation, \n",
        "  # search across 100 different combinations, and use all available cores\n",
        "  svc_search = RandomizedSearchCV(estimator = estimator, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=42, n_jobs = -1)\n",
        "  # Fit the random search model\n",
        "  svc_search.fit(train_X, train_Y)\n",
        "  \n",
        "  filename = 'drive/data/SVC_search_model.pkl'\n",
        "  with open(pkl_filename, 'wb') as file:\n",
        "    pickle.dump(svc_search, file)\n",
        "  \n",
        "  pkl_filename = 'drive/data/SVC_search_model.pkl' \n",
        "  with open(pkl_filename, 'rb') as file:\n",
        "    svc_search_model = pickle.load(file)\n",
        "    \n",
        "  print(\"_\"*30)\n",
        "  print(\"[RESULTS] After tuning\")\n",
        "  print('best hyperparameters:')\n",
        "  pprint(svc_search_model.best_params_)\n",
        "  # use these model to train the model again or :\n",
        "  best_model = svc_search_model.best_estimator_\n",
        "  best_accuracy = evaluate(best_model, test_X, test_Y)\n",
        "  print('accuracy = {}'.format(best_accuracy))\n",
        "    \n",
        "  return svc_search"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhcfSfbNFybN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ]
    },
    {
      "metadata": {
        "id": "sOKd5RJNF2AX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import math\n",
        "\n",
        "def inference(imgs_test, model_path, results_dir, h_neighbors):\n",
        "  \n",
        "  print(\"_\"*30)\n",
        "  print('[INFO] Doing inference on test images...')\n",
        "  \n",
        "  print('loading trained model.')\n",
        "  model = pickle.load(open(model_path, 'rb'))\n",
        "  \n",
        "  border = int((h_neighbors-1)/2)\n",
        "  \n",
        "  print('infer for each image')\n",
        "  for img in imgs_test:\n",
        "    img = cv2.copyMakeBorder(img, top=border, bottom=border, \\\n",
        "                                  left=border, right=border, \\\n",
        "                                  borderType = cv2.BORDER_CONSTANT, \\\n",
        "                                  value=[0, 0, 0])\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    features, _ = extract_features(img, img_gary, label=None, train=False)\n",
        "    \n",
        "    prediction = model.predict(features.reshape((-1, features.shape[1])))\n",
        "    size_pred = int(math.sqrt(features.shape[0]))\n",
        "    pred_img = prediction.reshape(size_pred, size_pred)\n",
        "    cv2.imwrite(results_dir+'/'+file.split('/')[-1], pred_img)\n",
        "  \n",
        "  print(\"_\"*30)\n",
        "  print('[INFO] Inference complete.')\n",
        "  print('show the last test image and its prediction:')\n",
        "\n",
        "  f, ax = plt.subplots(1,2)\n",
        "  ax[0].imshow(img)\n",
        "  ax[0].set_title('test image')\n",
        "  ax[1].imshow(pred_img)\n",
        "  ax[1].set_title('prediction')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IEHf8RwYJNBF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "inference(imgs_test, model_path = 'drive/data/RF_model.pkl' , results_dir='drive/data/results', h_neighbors = best_h_neighbors)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}